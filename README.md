# GWG_release - SYDE 675 Analysis
An extension of code which supplements the below paper
["Oops I Took A Gradient: Scalable Sampling for Discrete Distributions"](https://arxiv.org/abs/2102.04509) 
which has been accepted for a long presentation to ICML 2021. 

The paper is by [Will Grathwohl](http://www.cs.toronto.edu/~wgrathwohl/), Kevin Swersky, Milad Hashemi, 
[David Duvenaud](http://www.cs.toronto.edu/~duvenaud/), and [Chris Maddison](https://www.cs.toronto.edu/~cmaddis/).
The original repository can be found [here](https://github.com/wgrathwohl/GWG_release). It does not include a training dataset for protein contact analysis,
hence, my code reconstructs the dataset used to my best possible ability (simultaneously improving its flexibility
and enabling the generation of a larger training set).

I have adapted this code for the purposes of my SYDE 675 - Pattern Recogntion Final Project.
My contributions include protein_analysis_pipeline.py, which imports and/or repurposes many functions written by the 
original authors.
I also make significant use of the evcouplings python package (which is based on 
[this](https://doi.org/10.1371/journal.pone.0028766) paper by Marks et al.) in my train_logreg file, which generates 
features for my own choice of original model/feature combination.

# Requirements
*IMPORTANT: Use the provided OneDrive Link to fill the corresponding empty folder with files to speed up testing:
aln/ 2o72_test/ 1hzx_test/ logreg_test/
This work will require several python modules to work properly, which the original authors did not include. Hence, I 
generated a requirements.txt. After cloning the repository and creating a virtualenv if desired:
```
cd GWG_release
pip install -r requirements.txt
```
If you want to test the pipeline on an arbitrary protein, you will have to download hmmer as well as a sequence database 
in order to generate MSAs. It is expected by default in a databases folder in this repo and is >32GB compressed:
```
sudo apt install hmmer
mkdir databases
cd databases
wget https://ftp.uniprot.org/pub/databases/uniprot/current_release/uniref/uniref90/uniref90.fasta.gz
gzip -d uniref90.fasta.gz
```
# Training Data
It is not necessary to have the raw (multiple sequence alignment) training data for the Logistic Regression 
as the features have been precomputed and stored (shared via OneDrive aln/ folder). However, it can be downloaded like 
this, and then run through the generate_train_dataset function in train_logreg.py:
```
wget http://bioinfadmin.cs.ucl.ac.uk/downloads/contact_pred_datasets/dc_train
tar -xzf dc_train
```
It is sourced from the following paper: 
[Jones DT and Kandathil SM (2018). High precision in protein contact prediction using fully convolutional neural networks and minimal sequence features. Bioinformatics 34(19): 3308-3315.](https://github.com/psipred/DeepCov)
Contact maps are generated independently of this dataset, and so only the aln folder is needed.
Generating features from the training data is an enormous task, and so pickles with pre-extracted features are provided
via OneDrive, while the precomputed_training_data.pkl stores an example slice which can be loaded into memory.

# Experimenting with Code
protein_analysis_pipeline.py trains a Dense Potts model for contact prediction, using the Gibbs-with-Gradients algorithm
to sample the model, accelerating training. It should be possible to train a model for an arbitrary experimentally
solved structure from the pdb, provided you have the four-character code and chain corresponding to an entry in the
Protein DataBank: https://www.rcsb.org/
```
python protein_analysis_pipeline.py --pdb_code 2o72 --save_dir ./2o72_test
```
You can skip the computation of the MSA and contact map by copying the contents of the 2o72_test or other corresponding
folder locally, as long as it is specified as the --save_dir.
The train_logreg.py file will use a pickle to recieve the input training data by default. You can just call:
```
python train_logreg.py --save_dir ./logreg_test --aln_folder_location ./aln --precomputed --n_test 1
```
To test the model training pipeline. This script will expect at least the specified number of test examples to be in 
the aln folder: these must be downloaded from the aln folder in OneDrive, and each tested protein should have a .txt and 
.pkl file.
Any files in this repo not mentioned above were generated by the original authors.
